import shutil
from collections import defaultdict
from pathlib import Path

import numpy as np


class YOLODatasetSplitter:
    """
    Chia táº­p dá»¯ liá»‡u YOLO detection sao cho Ä‘á»u vá»:
    - PhÃ¢n bá»‘ classes
    - Sá»‘ lÆ°á»£ng boxes
    - Sá»‘ lÆ°á»£ng images
    
    Format YOLO: má»—i dÃ²ng trong file .txt lÃ  [class_id x_center y_center width height]
    """
    
    def __init__(self, val_ratio=0.2, random_seed=42):
        """
        Args:
            val_ratio: tá»‰ lá»‡ validation (0.2 = 20%)
            random_seed: seed Ä‘á»ƒ reproducible
        """
        self.val_ratio = val_ratio
        self.random_seed = random_seed
        np.random.seed(random_seed)
    
    def parse_yolo_label(self, label_path):
        """
        Äá»c file label YOLO format
        Returns: list of [class_id, x, y, w, h]
        """
        annotations = []
        try:
            with open(label_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if line:
                        parts = line.split()
                        class_id = int(parts[0])
                        bbox = [float(x) for x in parts[1:5]]
                        annotations.append([class_id] + bbox)
        except FileNotFoundError:
            pass  # File khÃ´ng cÃ³ annotations
        return annotations
    
    def load_dataset(self, images_dir, labels_dir):
        """
        Load toÃ n bá»™ dataset tá»« thÆ° má»¥c images vÃ  labels
        
        Args:
            images_dir: thÆ° má»¥c chá»©a áº£nh
            labels_dir: thÆ° má»¥c chá»©a file labels .txt
        
        Returns:
            dict: {image_filename: [annotations]}
        """
        images_dir = Path(images_dir)
        labels_dir = Path(labels_dir)
        
        dataset = {}
        
        # TÃ¬m táº¥t cáº£ áº£nh
        image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']
        for img_path in images_dir.iterdir():
            if img_path.suffix.lower() in image_extensions:
                # TÃ¬m file label tÆ°Æ¡ng á»©ng
                label_path = labels_dir / f"{img_path.stem}.txt"
                annotations = self.parse_yolo_label(label_path)
                dataset[img_path.name] = annotations
        
        print(f"âœ“ ÄÃ£ load {len(dataset)} images tá»« {images_dir}")
        return dataset
    
    def analyze_dataset(self, dataset):
        """
        PhÃ¢n tÃ­ch dataset Ä‘á»ƒ biáº¿t phÃ¢n bá»‘ classes vÃ  boxes
        """
        stats = {
            'total_images': len(dataset),
            'total_boxes': 0,
            'class_distribution': defaultdict(int),
            'boxes_per_image': [],
            'images_per_class': defaultdict(list)
        }
        
        for img_name, annotations in dataset.items():
            n_boxes = len(annotations)
            stats['total_boxes'] += n_boxes
            stats['boxes_per_image'].append(n_boxes)
            
            classes_in_img = set()
            for ann in annotations:
                class_id = ann[0]
                stats['class_distribution'][class_id] += 1
                classes_in_img.add(class_id)
            
            for class_id in classes_in_img:
                stats['images_per_class'][class_id].append(img_name)
        
        return stats
    
    def stratified_split(self, dataset):
        """
        Chia dá»¯ liá»‡u theo phÆ°Æ¡ng phÃ¡p stratified sampling
        Ä‘áº£m báº£o phÃ¢n bá»‘ Ä‘á»u cÃ¡c classes
        """
        # Táº¡o list cÃ¡c image vá»›i thÃ´ng tin vá» classes
        image_info = []
        for img_name, annotations in dataset.items():
            classes = set([ann[0] for ann in annotations])
            n_boxes = len(annotations)
            image_info.append({
                'name': img_name,
                'classes': classes,
                'n_boxes': n_boxes
            })
        
        # Sáº¯p xáº¿p theo sá»‘ classes vÃ  sá»‘ boxes Ä‘á»ƒ phÃ¢n bá»‘ Ä‘á»u
        image_info.sort(key=lambda x: (len(x['classes']), x['n_boxes']))
        
        # Stratified sampling
        train_imgs = []
        val_imgs = []
        
        # Track sá»‘ lÆ°á»£ng má»—i class Ä‘Ã£ Ä‘Æ°á»£c chia
        train_class_count = defaultdict(int)
        val_class_count = defaultdict(int)
        train_box_count = 0
        val_box_count = 0
        
        for img in image_info:
            img_annotations = dataset[img['name']]
            img_boxes = len(img_annotations)
            
            if len(train_imgs) == 0:
                # Image Ä‘áº§u tiÃªn vÃ o train
                train_imgs.append(img['name'])
                train_box_count += img_boxes
                for class_id in img['classes']:
                    train_class_count[class_id] += sum(
                        1 for ann in img_annotations if ann[0] == class_id
                    )
            else:
                # TÃ­nh tá»‰ lá»‡ hiá»‡n táº¡i
                current_val_ratio = len(val_imgs) / (len(train_imgs) + len(val_imgs))
                
                # TÃ­nh score Ä‘á»ƒ quyáº¿t Ä‘á»‹nh train hay val
                val_score = 0
                train_score = 0
                
                for class_id in img['classes']:
                    train_total = train_class_count[class_id]
                    val_total = val_class_count[class_id]
                    total = train_total + val_total
                    
                    if total > 0:
                        current_train_ratio = train_total / total
                        current_val_ratio_cls = val_total / total
                        
                        # Æ¯u tiÃªn thÃªm vÃ o táº­p nÃ o Ä‘ang thiáº¿u class nÃ y
                        val_score += abs((1 - self.val_ratio) - current_train_ratio)
                        train_score += abs(self.val_ratio - current_val_ratio_cls)
                
                # Quyáº¿t Ä‘á»‹nh dá»±a trÃªn tá»‰ lá»‡ vÃ  score
                if current_val_ratio < self.val_ratio or val_score < train_score:
                    val_imgs.append(img['name'])
                    val_box_count += img_boxes
                    for ann in img_annotations:
                        val_class_count[ann[0]] += 1
                else:
                    train_imgs.append(img['name'])
                    train_box_count += img_boxes
                    for ann in img_annotations:
                        train_class_count[ann[0]] += 1
        
        return train_imgs, val_imgs, train_class_count, val_class_count
    
    def print_statistics(self, dataset, train_imgs, val_imgs, 
                        train_class_count, val_class_count):
        """
        In thá»‘ng kÃª vá» cÃ¡ch chia dá»¯ liá»‡u
        """
        print("\n" + "="*70)
        print("THá»NG KÃŠ CHIA Dá»® LIá»†U YOLO")
        print("="*70)
        
        # Thá»‘ng kÃª tá»•ng quan
        total = len(train_imgs) + len(val_imgs)
        print(f"\nðŸ“Š Tá»•ng sá»‘ images: {total}")
        print(f"   Train: {len(train_imgs):>6} ({len(train_imgs)/total*100:>5.1f}%)")
        print(f"   Val:   {len(val_imgs):>6} ({len(val_imgs)/total*100:>5.1f}%)")
        
        # Thá»‘ng kÃª boxes
        train_boxes = sum(len(dataset[img]) for img in train_imgs)
        val_boxes = sum(len(dataset[img]) for img in val_imgs)
        total_boxes = train_boxes + val_boxes
        
        print(f"\nðŸ“¦ Tá»•ng sá»‘ boxes: {total_boxes}")
        print(f"   Train: {train_boxes:>6} ({train_boxes/total_boxes*100:>5.1f}%)")
        print(f"   Val:   {val_boxes:>6} ({val_boxes/total_boxes*100:>5.1f}%)")
        
        # Thá»‘ng kÃª theo class
        print(f"\nðŸ“‹ PhÃ¢n bá»‘ theo classes:")
        print(f"{'Class':<10} {'Train':<20} {'Val':<20} {'Total':<10}")
        print("-" * 70)
        
        all_classes = sorted(set(train_class_count.keys()) | set(val_class_count.keys()))
        for cls in all_classes:
            train_cnt = train_class_count.get(cls, 0)
            val_cnt = val_class_count.get(cls, 0)
            total_cnt = train_cnt + val_cnt
            
            if total_cnt > 0:
                print(f"{cls:<10} {train_cnt:>6} ({train_cnt/total_cnt*100:>5.1f}%) "
                      f"    {val_cnt:>6} ({val_cnt/total_cnt*100:>5.1f}%) "
                      f"    {total_cnt:<10}")
    
    def split_and_organize(self, images_dir, labels_dir, output_dir='dataset_split',
                          copy_files=True):
        """
        Chia dá»¯ liá»‡u vÃ  tá»• chá»©c theo cáº¥u trÃºc YOLO
        
        Args:
            images_dir: thÆ° má»¥c chá»©a áº£nh gá»‘c
            labels_dir: thÆ° má»¥c chá»©a labels gá»‘c
            output_dir: thÆ° má»¥c output
            copy_files: True Ä‘á»ƒ copy files, False Ä‘á»ƒ chá»‰ táº¡o file lists
        """
        images_dir = Path(images_dir)
        labels_dir = Path(labels_dir)
        output_dir = Path(output_dir)
        
        # Load dataset
        dataset = self.load_dataset(images_dir, labels_dir)
        
        if len(dataset) == 0:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u!")
            return None, None
        
        # Chia dá»¯ liá»‡u
        train_imgs, val_imgs, train_class_count, val_class_count = \
            self.stratified_split(dataset)
        
        # In thá»‘ng kÃª
        self.print_statistics(dataset, train_imgs, val_imgs,
                            train_class_count, val_class_count)
        

        with open('datasets/process/train.txt', 'w') as f:
            for name in train_imgs:
                path = f'datasets/process/images/{name}'
                f.write(f'{path}\n')
        with open('datasets/process/val.txt', 'w') as f:
            for name in val_imgs:
                path = f'datasets/process/images/{name}'
                f.write(f'{path}\n')

        print(f"\nðŸŽ‰ HoÃ n táº¥t!")
        
        return train_imgs, val_imgs


# VÃ­ dá»¥ sá»­ dá»¥ng
if __name__ == "__main__":
    # CÃ¡ch 1: Chá»‰ táº¡o file lists (khÃ´ng copy files)
    splitter = YOLODatasetSplitter(val_ratio=0.2, random_seed=42)
    train_imgs, val_imgs = splitter.split_and_organize(
        images_dir='datasets/process/images',
        labels_dir='datasets/process/labels',
        output_dir='dataset_split',
        copy_files=False  # Chá»‰ táº¡o train.txt vÃ  val.txt
    )
    
    # CÃ¡ch 2: Copy files vÃ  tá»• chá»©c theo cáº¥u trÃºc YOLO
    # train_imgs, val_imgs = splitter.split_and_organize(
    #     images_dir='path/to/images',
    #     labels_dir='path/to/labels',
    #     output_dir='dataset_split',
    #     copy_files=True  # Copy files vÃ o thÆ° má»¥c train/val
    # )